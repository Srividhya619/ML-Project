{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9UUJsyzI5i8",
        "outputId": "10aa4b9c-0ab7-4a78-d5aa-56e6c9b6a72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.12/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.12/dist-packages (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.12/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.10.5)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.12/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.12/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.12/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from deep-translator) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas googletrans==4.0.0-rc1 deep-translator tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas tqdm sklearn deep-translator google-cloud-translate==3.11.2 sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilh5RCWjOpOe",
        "outputId": "3ff3a6bd-afe1-459c-809e-1843d96a11a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import uuid\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Choose translator backend: \"deep_translator\" ---\n",
        "TRANSLATOR_BACKEND = \"deep_translator\"\n",
        "\n",
        "SMALL_PATH = \"mentalhealth.csv\"   # FAQ: Question_ID, Questions, Answers\n",
        "LARGE_PATH = \"train.csv\"   # Dialogue: Context, Response\n",
        "OUT_DIR = Path(\"output\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "MERGED_EN_PATH = OUT_DIR / \"merged_en.csv\"\n",
        "MERGED_MULTILANG_PATH = OUT_DIR / \"merged_multilang.csv\"\n",
        "MERGED_JSONL_PATH = OUT_DIR / \"merged_multilang.jsonl\"\n",
        "\n",
        "# Translation settings\n",
        "TARGET_LANGS = [\"hi\", \"te\"]  # Hindi, Telugu\n",
        "BATCH_SIZE = 16              # batch size for translation calls (tuned per backend)\n",
        "SLEEP_BETWEEN_BATCHES = 0.5  # seconds to reduce rate-limit risk\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# Helper utilities\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    s = str(text)\n",
        "    s = s.replace(\"\\u00A0\", \" \")\n",
        "    s = s.replace(\"Â\", \"\").replace(\"â€™\", \"'\").replace(\"â€œ\", '\"').replace(\"â€\", '\"')\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def make_id(prefix=\"u\"):\n",
        "    return f\"{prefix}_{uuid.uuid4().hex[:12]}\"\n",
        "\n",
        "def stable_fingerprint(s, maxlen=2000):\n",
        "    if s is None:\n",
        "        s = \"\"\n",
        "    s = clean_text(s).lower()\n",
        "    s = s[:maxlen]\n",
        "    return hashlib.md5(s.encode(\"utf8\")).hexdigest()\n",
        "\n",
        "\n",
        "# 1) Load & normalize datasets\n",
        "def load_small(path):\n",
        "    # Try TSV/CSV auto-detection\n",
        "    p = Path(path)\n",
        "    if p.suffix in [\".tsv\", \".txt\"]:\n",
        "        df = pd.read_csv(path, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
        "    else:\n",
        "        try:\n",
        "            df = pd.read_csv(path, dtype=str, keep_default_na=False)\n",
        "        except:\n",
        "            df = pd.read_csv(path, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
        "\n",
        "    # detect columns\n",
        "    qid_col = next((c for c in df.columns if c.lower().startswith(\"question_id\")), None)\n",
        "    q_col   = next((c for c in df.columns if \"question\" in c.lower() and \"id\" not in c.lower()), None)\n",
        "    a_col   = next((c for c in df.columns if \"answer\" in c.lower()), None)\n",
        "\n",
        "    if not (q_col and a_col):\n",
        "        raise ValueError(f\"Couldn't detect question/answer columns in {path}. Columns: {list(df.columns)}\")\n",
        "\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        qid = r[qid_col] if qid_col and qid_col in r.index else None\n",
        "        qtxt = clean_text(r[q_col])\n",
        "        atxt = clean_text(r[a_col])\n",
        "        rows.append({\n",
        "            \"id\": f\"faq_{qid}\" if qid and str(qid).strip() else make_id(\"faq\"),\n",
        "            \"source\": p.name,\n",
        "            \"type\": \"faq\",\n",
        "            \"prompt\": qtxt,\n",
        "            \"response\": atxt,\n",
        "            \"lang\": \"en\",\n",
        "            \"orig_id\": qid\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def load_large(path):\n",
        "    p = Path(path)\n",
        "    # robust CSV reading even if responses have newlines; user should ensure correct quoting in CSV\n",
        "    df = pd.read_csv(path, dtype=str, keep_default_na=False)\n",
        "    ctx_col = next((c for c in df.columns if \"context\" in c.lower()), None)\n",
        "    resp_col = next((c for c in df.columns if \"response\" in c.lower() or \"reply\" in c.lower()), None)\n",
        "    if not (ctx_col and resp_col):\n",
        "        raise ValueError(f\"Couldn't detect Context/Response columns in {path}. Columns: {list(df.columns)}\")\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        ctx = clean_text(r[ctx_col])\n",
        "        resp = clean_text(r[resp_col])\n",
        "        fid = stable_fingerprint(ctx + \"||\" + resp)\n",
        "        rows.append({\n",
        "            \"id\": f\"dlg_{fid}\",\n",
        "            \"source\": p.name,\n",
        "            \"type\": \"dialogue\",\n",
        "            \"prompt\": ctx,\n",
        "            \"response\": resp,\n",
        "            \"lang\": \"en\",\n",
        "            \"orig_id\": None\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "df_small = load_small(SMALL_PATH)\n",
        "df_large = load_large(LARGE_PATH)\n",
        "merged = pd.concat([df_small, df_large], ignore_index=True, sort=False)\n",
        "print(\"Initial merged rows:\", len(merged))\n",
        "\n",
        "# Deduplicate (exact prompt+response fingerprint)\n",
        "merged[\"fp\"] = merged.apply(lambda r: stable_fingerprint((r[\"prompt\"] or \"\") + \"||\" + (r[\"response\"] or \"\")), axis=1)\n",
        "before = len(merged)\n",
        "merged = merged.drop_duplicates(subset=[\"fp\"]).reset_index(drop=True)\n",
        "after = len(merged)\n",
        "print(f\"Deduped: {before} -> {after}\")\n",
        "\n",
        "# Drop empty prompts/responses\n",
        "merged = merged[~((merged[\"prompt\"].str.strip()==\"\") | (merged[\"response\"].str.strip()==\"\"))].reset_index(drop=True)\n",
        "print(\"After dropping empty rows:\", len(merged))\n",
        "\n",
        "# Ensure id exists\n",
        "def ensure_id(row):\n",
        "    if row[\"id\"] and str(row[\"id\"]).strip():\n",
        "        return row[\"id\"]\n",
        "    return f\"u_{row['fp'][:12]}\"\n",
        "merged[\"id\"] = merged.apply(ensure_id, axis=1)\n",
        "\n",
        "# Keep columns order\n",
        "merged = merged[[\"id\",\"source\",\"type\",\"prompt\",\"response\",\"lang\",\"orig_id\",\"fp\"]]\n",
        "\n",
        "# Save merged English\n",
        "merged.to_csv(MERGED_EN_PATH, index=False)\n",
        "print(\"Saved merged English to\", MERGED_EN_PATH)\n",
        "\n",
        "\n",
        "# 2) Translation helpers (two backends)\n",
        "if TRANSLATOR_BACKEND == \"google_cloud\":\n",
        "    # Google Cloud translate v3\n",
        "    from google.cloud import translate_v3 as translate\n",
        "    client = translate.TranslationServiceClient()\n",
        "    GCP_PROJECT = \"YOUR-GCP-PROJECT-ID\"\n",
        "    LOCATION = \"global\"\n",
        "    parent = f\"projects/{GCP_PROJECT}/locations/{LOCATION}\"\n",
        "\n",
        "    def gc_translate_batch(texts, target_lang):\n",
        "        # Google can take a list of contents\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                response = client.translate_text(\n",
        "                    request={\n",
        "                        \"parent\": parent,\n",
        "                        \"contents\": texts,\n",
        "                        \"mime_type\": \"text/plain\",\n",
        "                        \"target_language_code\": target_lang\n",
        "                    }\n",
        "                )\n",
        "                return [r.translated_text for r in response.translations]\n",
        "            except Exception as e:\n",
        "                wait = 2**attempt\n",
        "                print(f\"GC translate error: {e}. retrying in {wait}s\")\n",
        "                time.sleep(wait)\n",
        "        # fallback: return originals\n",
        "        return texts\n",
        "\n",
        "    def translate_batch(texts, target_lang):\n",
        "        # split into chunks that are not too large\n",
        "        out = []\n",
        "        for i in range(0, len(texts), BATCH_SIZE):\n",
        "            batch = texts[i:i+BATCH_SIZE]\n",
        "            out.extend(gc_translate_batch(batch, target_lang))\n",
        "            time.sleep(SLEEP_BETWEEN_BATCHES)\n",
        "        return out\n",
        "\n",
        "elif TRANSLATOR_BACKEND == \"deep_translator\":\n",
        "    from deep_translator import GoogleTranslator as DeepGoogle\n",
        "    # deep-translator will call translate.google.com under the hood\n",
        "    def deep_translate_one(text, target_lang):\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                return DeepGoogle(source='auto', target=target_lang).translate(text)\n",
        "            except Exception as e:\n",
        "                wait = 2**attempt\n",
        "                # sometimes transient; backoff\n",
        "                time.sleep(wait)\n",
        "        return text\n",
        "\n",
        "    def translate_batch(texts, target_lang):\n",
        "        out = []\n",
        "        for i in tqdm(range(0, len(texts), 1), desc=f\"Translating to {target_lang}\"):\n",
        "            t = texts[i]\n",
        "            out.append(deep_translate_one(t, target_lang))\n",
        "            # small sleep to reduce chance of rate limiting\n",
        "            if i % 10 == 0:\n",
        "                time.sleep(0.2)\n",
        "        return out\n",
        "else:\n",
        "    raise ValueError(\"Unsupported TRANSLATOR_BACKEND: choose 'google_cloud' or 'deep_translator'\")\n",
        "\n",
        "\n",
        "# 3) Translate merged dataset into target langs\n",
        "def batch_translate_df(df, target_langs):\n",
        "    # Returns a dataframe with original rows plus translations (rows repeated with same id, lang changed)\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "    for lang in target_langs:\n",
        "        print(f\"Translating to {lang} ...\")\n",
        "        prompts = df[\"prompt\"].tolist()\n",
        "        responses = df[\"response\"].tolist()\n",
        "        # translate in two passes (prompt and response)\n",
        "        t_prompts = translate_batch(prompts, lang)\n",
        "        t_responses = translate_batch(responses, lang)\n",
        "        # cleaned translations\n",
        "        t_prompts = [clean_text(t) for t in t_prompts]\n",
        "        t_responses = [clean_text(t) for t in t_responses]\n",
        "\n",
        "        for i in range(n):\n",
        "            base = df.iloc[i]\n",
        "            rows.append({\n",
        "                \"id\": base[\"id\"],\n",
        "                \"source\": base[\"source\"],\n",
        "                \"type\": base[\"type\"],\n",
        "                \"prompt\": t_prompts[i],\n",
        "                \"response\": t_responses[i],\n",
        "                \"lang\": lang,\n",
        "                \"orig_id\": base[\"orig_id\"],\n",
        "                \"fp\": stable_fingerprint(t_prompts[i] + \"||\" + t_responses[i])\n",
        "            })\n",
        "        # brief rest to reduce risk\n",
        "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
        "\n",
        "    # Also include original English rows\n",
        "    en_rows = df.to_dict(orient=\"records\")\n",
        "    # convert en_rows to same schema naming expected\n",
        "    en_formatted = []\n",
        "    for r in en_rows:\n",
        "        en_formatted.append({\n",
        "            \"id\": r[\"id\"],\n",
        "            \"source\": r[\"source\"],\n",
        "            \"type\": r[\"type\"],\n",
        "            \"prompt\": r[\"prompt\"],\n",
        "            \"response\": r[\"response\"],\n",
        "            \"lang\": \"en\",\n",
        "            \"orig_id\": r[\"orig_id\"],\n",
        "            \"fp\": r[\"fp\"]\n",
        "        })\n",
        "\n",
        "    all_rows = en_formatted + rows\n",
        "    out_df = pd.DataFrame(all_rows)\n",
        "    return out_df\n",
        "\n",
        "print(\"Translating merged dataset into target languages:\", TARGET_LANGS)\n",
        "multilang = batch_translate_df(merged, TARGET_LANGS)\n",
        "print(\"Multilang rows:\", len(multilang))\n",
        "multilang.to_csv(MERGED_MULTILANG_PATH, index=False)\n",
        "print(\"Saved multilingual merged file:\", MERGED_MULTILANG_PATH)\n",
        "\n",
        "# Save JSONL for fine-tuning (prompt/response pairs grouped by language)\n",
        "import json\n",
        "with open(MERGED_JSONL_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
        "    for _, r in multilang.iterrows():\n",
        "        # For model fine-tuning JSONL style: {\"prompt\":\"...\",\"completion\":\"...\"}\n",
        "        obj = {\"id\": r[\"id\"], \"lang\": r[\"lang\"], \"prompt\": r[\"prompt\"], \"response\": r[\"response\"]}\n",
        "        fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "print(\"Saved JSONL to\", MERGED_JSONL_PATH)\n",
        "\n",
        "print(\"All finished. Files in\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VttRHpqOsXc",
        "outputId": "a68729a6-099b-4e4a-cd16-c825c4aa0db2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Initial merged rows: 3609\n",
            "Deduped: 3609 -> 2125\n",
            "After dropping empty rows: 2121\n",
            "Saved merged English to output/merged_en.csv\n",
            "Translating merged dataset into target languages: ['hi', 'te']\n",
            "Translating to hi ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating to hi: 100%|██████████| 2121/2121 [12:57<00:00,  2.73it/s]\n",
            "Translating to hi: 100%|██████████| 2121/2121 [39:04<00:00,  1.11s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating to te ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating to te: 100%|██████████| 2121/2121 [20:39<00:00,  1.71it/s]\n",
            "Translating to te: 100%|██████████| 2121/2121 [48:27<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multilang rows: 6363\n",
            "Saved multilingual merged file: output/merged_multilang.csv\n",
            "Saved JSONL to output/merged_multilang.jsonl\n",
            "All finished. Files in output\n"
          ]
        }
      ]
    }
  ]
}