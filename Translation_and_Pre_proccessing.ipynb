{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv_-9Z31pru-",
        "outputId": "5bb86546-2680-4169-830f-08ff41027e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas scikit-learn deep-translator tqdm transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deep_translator import GoogleTranslator as DeepGoogle\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries are imported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJHcc-Cgp28w",
        "outputId": "b6ddc5a9-2cdc-455b-a467-26094c40aa64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries are imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "yY7pHnf1qU8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SMALL_PATH = \"mentalhealth.csv\"\n",
        "LARGE_PATH = \"train.csv\"\n",
        "\n",
        "# Output directory\n",
        "OUT_DIR = Path(\"output\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Output files\n",
        "MERGED_EN_PATH = OUT_DIR / \"merged_en.csv\"\n",
        "MERGED_MULTILANG_PATH = OUT_DIR / \"merged_multilang.csv\"\n",
        "MERGED_JSONL_PATH = OUT_DIR / \"merged_multilang.jsonl\"\n",
        "\n",
        "# Translation settings\n",
        "TARGET_LANGS = [\"hi\", \"te\"]  # Hindi, Telugu\n",
        "SLEEP_BETWEEN_TRANSLATIONS = 0.3  # Seconds (rate limiting)\n",
        "MAX_RETRIES = 5\n",
        "\n",
        "# Cleaning settings\n",
        "MIN_CHAR_LENGTH = 2\n",
        "MAX_CHAR_LENGTH = 1000\n",
        "REMOVE_URLS = False\n",
        "\n",
        "print(\"Configuration loaded\")\n",
        "print(f\"Input files: {SMALL_PATH}, {LARGE_PATH}\")\n",
        "print(f\"Output directory: {OUT_DIR}\")\n",
        "print(f\"Target languages: {TARGET_LANGS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u36VoaIcqT5f",
        "outputId": "5e617017-67b1-49dc-9ccb-f1964e1c4f9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded\n",
            "Input files: mentalhealth.csv, train.csv\n",
            "Output directory: output\n",
            "Target languages: ['hi', 'te']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning and Filtering"
      ],
      "metadata": {
        "id": "kyLpHkrgrEAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_filter_dataset(df,\n",
        "                             min_char_length=2,\n",
        "                             max_char_length=1000,\n",
        "                             remove_urls=False,\n",
        "                             verbose=True):\n",
        "    if verbose:\n",
        "        print(f\"CLEANING DATASET\")\n",
        "        print(f\"Initial rows: {len(df)}\")\n",
        "        start = time.time()\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Convert to string and basic normalization\n",
        "    if verbose:\n",
        "        print(\"  â†’ Converting to string and normalizing...\")\n",
        "\n",
        "    df_clean['prompt'] = df_clean['prompt'].astype(str).str.strip()\n",
        "    df_clean['response'] = df_clean['response'].astype(str).str.strip()\n",
        "\n",
        "    # Fix Unicode artifacts\n",
        "    replacements = {\n",
        "        \"\\u00A0\": \" \",\n",
        "        \"Ã‚\": \"\",\n",
        "        \"Ã¢â‚¬â„¢\": \"'\",\n",
        "        \"Ã¢â‚¬Å“\": '\"',\n",
        "        \"Ã¢â‚¬\": '\"',\n",
        "        'Ã¢â‚¬\"': \"-\",\n",
        "    }\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        df_clean['prompt'] = df_clean['prompt'].str.replace(old, new, regex=False)\n",
        "        df_clean['response'] = df_clean['response'].str.replace(old, new, regex=False)\n",
        "\n",
        "    # Normalize multiple spaces/newlines to single space\n",
        "    df_clean['prompt'] = df_clean['prompt'].str.replace(r'\\s+', ' ', regex=True)\n",
        "    df_clean['response'] = df_clean['response'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "    # Remove URLs\n",
        "    if remove_urls:\n",
        "        if verbose:\n",
        "            print(\"  â†’ Removing URLs...\")\n",
        "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "        df_clean['prompt'] = df_clean['prompt'].str.replace(url_pattern, '', regex=True)\n",
        "        df_clean['response'] = df_clean['response'].str.replace(url_pattern, '', regex=True)\n",
        "\n",
        "        # Remove emails\n",
        "        df_clean['prompt'] = df_clean['prompt'].str.replace(r'\\S+@\\S+', '', regex=True)\n",
        "        df_clean['response'] = df_clean['response'].str.replace(r'\\S+@\\S+', '', regex=True)\n",
        "\n",
        "        # Re-normalize after removal\n",
        "        df_clean['prompt'] = df_clean['prompt'].str.strip()\n",
        "        df_clean['response'] = df_clean['response'].str.strip()\n",
        "\n",
        "    # Length-based filtering\n",
        "    if verbose:\n",
        "        print(\"  â†’ Filtering by length...\")\n",
        "\n",
        "    initial = len(df_clean)\n",
        "\n",
        "    # Calculate lengths once\n",
        "    prompt_lens = df_clean['prompt'].str.len()\n",
        "    response_lens = df_clean['response'].str.len()\n",
        "\n",
        "    # Create mask for valid entries\n",
        "    valid_mask = (\n",
        "        (prompt_lens >= min_char_length) &\n",
        "        (prompt_lens <= max_char_length) &\n",
        "        (response_lens >= min_char_length) &\n",
        "        (response_lens <= max_char_length) &\n",
        "        (df_clean['prompt'] != 'nan') &      # Filter 'nan' strings\n",
        "        (df_clean['response'] != 'nan') &\n",
        "        (df_clean['prompt'] != 'None') &     # Filter 'None' strings\n",
        "        (df_clean['response'] != 'None')\n",
        "    )\n",
        "\n",
        "    df_clean = df_clean[valid_mask].copy()\n",
        "\n",
        "    # Reset index\n",
        "    df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "    if verbose:\n",
        "        elapsed = time.time() - start\n",
        "        removed = initial - len(df_clean)\n",
        "        pct_removed = (removed / initial * 100) if initial > 0 else 0\n",
        "        pct_kept = ((len(df_clean) / len(df)) * 100) if len(df) > 0 else 0\n",
        "\n",
        "        print(f\"\\nCLEANING COMPLETE!\")\n",
        "        print(f\"Final rows: {len(df_clean)}\")\n",
        "        print(f\"Removed: {removed} ({pct_removed:.1f}%)\")\n",
        "        print(f\"Retained: {pct_kept:.1f}% of original\")\n",
        "        print(f\"Time: {elapsed:.2f}s\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "print(\"Cleaning function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTaEeiSIq-Nh",
        "outputId": "40773923-e767-4811-92f7-1fa35a2b9fc1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading Functions"
      ],
      "metadata": {
        "id": "EbQPYyheuJFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stable_fingerprint(s, max_len=2000):\n",
        "    if s is None:\n",
        "        s = \"\"\n",
        "    s = str(s).lower()[:max_len]\n",
        "    return hashlib.md5(s.encode('utf-8')).hexdigest()\n",
        "\n",
        "def make_id(prefix=\"u\"):\n",
        "    return f\"{prefix}_{uuid.uuid4().hex[:12]}\"\n",
        "\n",
        "def load_small_dataset(path):\n",
        "    p = Path(path)\n",
        "\n",
        "    if p.suffix in ['.tsv', '.txt']:\n",
        "        df = pd.read_csv(path, sep='\\t', dtype=str, keep_default_na=False)\n",
        "    else:\n",
        "        try:\n",
        "            df = pd.read_csv(path, dtype=str, keep_default_na=False)\n",
        "        except:\n",
        "            df = pd.read_csv(path, sep='\\t', dtype=str, keep_default_na=False)\n",
        "\n",
        "    # Detect columns\n",
        "    qid_col = next((c for c in df.columns if c.lower().startswith('questionid')), None)\n",
        "    q_col = next((c for c in df.columns if 'question' in c.lower() and 'id' not in c.lower()), None)\n",
        "    a_col = next((c for c in df.columns if 'answer' in c.lower()), None)\n",
        "\n",
        "    if not (q_col and a_col):\n",
        "        raise ValueError(f\"Couldn't detect question/answer columns in {path}. Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Build standardized format\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        qid = r[qid_col] if qid_col and qid_col in r.index else None\n",
        "        q_txt = str(r[q_col]).strip()\n",
        "        a_txt = str(r[a_col]).strip()\n",
        "\n",
        "        rows.append({\n",
        "            'id': f\"faq_{qid}\" if qid and str(qid).strip() else make_id(\"faq\"),\n",
        "            'source': p.name,\n",
        "            'type': 'faq',\n",
        "            'prompt': q_txt,\n",
        "            'response': a_txt,\n",
        "            'lang': 'en',\n",
        "            'orig_id': qid\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def load_large_dataset(path):\n",
        "    p = Path(path)\n",
        "    df = pd.read_csv(path, dtype=str, keep_default_na=False)\n",
        "\n",
        "    # Detect columns\n",
        "    ctx_col = next((c for c in df.columns if 'context' in c.lower()), None)\n",
        "    resp_col = next((c for c in df.columns if 'response' in c.lower() or 'reply' in c.lower()), None)\n",
        "\n",
        "    if not (ctx_col and resp_col):\n",
        "        raise ValueError(f\"Couldn't detect Context/Response columns in {path}. Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Build standardized format\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        ctx = str(r[ctx_col]).strip()\n",
        "        resp = str(r[resp_col]).strip()\n",
        "        fid = stable_fingerprint(ctx + resp)\n",
        "\n",
        "        rows.append({\n",
        "            'id': f\"dlg_{fid}\",\n",
        "            'source': p.name,\n",
        "            'type': 'dialogue',\n",
        "            'prompt': ctx,\n",
        "            'response': resp,\n",
        "            'lang': 'en',\n",
        "            'orig_id': None\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "print(\"Data loading functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA-6iz2huIDy",
        "outputId": "18a0c2ea-3160-40dc-b6d3-602f83d670af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Merge Datasets"
      ],
      "metadata": {
        "id": "BSU7klX2uvvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--\" * 80)\n",
        "print(\"LOADING DATASETS\")\n",
        "print(\"--\" * 80)\n",
        "\n",
        "# Load datasets\n",
        "print(f\"\\nLoading {SMALL_PATH}...\")\n",
        "df_small = load_small_dataset(SMALL_PATH)\n",
        "print(f\"Loaded {len(df_small)} FAQ entries\")\n",
        "\n",
        "print(f\"\\nLoading {LARGE_PATH}...\")\n",
        "df_large = load_large_dataset(LARGE_PATH)\n",
        "print(f\"Loaded {len(df_large)} dialogue entries\")\n",
        "\n",
        "# Merge\n",
        "print(f\"\\nMerging datasets...\")\n",
        "merged = pd.concat([df_small, df_large], ignore_index=True, sort=False)\n",
        "print(f\"Initial merged rows: {len(merged)}\")\n",
        "\n",
        "# DEDUPLICATION\n",
        "print(\"\\n\" + \"--\" * 80)\n",
        "print(\"DEDUPLICATION\")\n",
        "print(\"--\" * 80)\n",
        "\n",
        "# Create fingerprints for deduplication\n",
        "merged['fp'] = merged.apply(\n",
        "    lambda r: stable_fingerprint((r['prompt'] or '') + (r['response'] or '')),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "before = len(merged)\n",
        "merged = merged.drop_duplicates(subset='fp').reset_index(drop=True)\n",
        "after = len(merged)\n",
        "print(f\"\\nRemoved {before - after} duplicates\")\n",
        "print(f\"Rows after dedup: {after}\")\n",
        "\n",
        "# CLEANING & FILTERING\n",
        "print(\"\\n\" + \"--\" * 80)\n",
        "print(\"CLEANING & FILTERING\")\n",
        "print(\"--\" * 80)\n",
        "\n",
        "merged = clean_and_filter_dataset(\n",
        "    merged,\n",
        "    min_char_length=MIN_CHAR_LENGTH,\n",
        "    max_char_length=MAX_CHAR_LENGTH,\n",
        "    remove_urls=REMOVE_URLS,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Ensure ID exists for all rows\n",
        "def ensure_id(row):\n",
        "    if row['id'] and str(row['id']).strip():\n",
        "        return row['id']\n",
        "    return f\"u_{row['fp'][:12]}\"\n",
        "\n",
        "merged['id'] = merged.apply(ensure_id, axis=1)\n",
        "\n",
        "# Keep only necessary columns\n",
        "merged = merged[['id', 'source', 'type', 'prompt', 'response', 'lang', 'orig_id', 'fp']]\n",
        "\n",
        "# Save cleaned English dataset\n",
        "merged.to_csv(MERGED_EN_PATH, index=False)\n",
        "print(f\"\\nSaved cleaned English dataset to: {MERGED_EN_PATH}\")\n",
        "print(f\"  Final row count: {len(merged)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample of cleaned data:\")\n",
        "print(merged[['prompt', 'response']].head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "steemhW4utvv",
        "outputId": "d5191236-039d-4a4e-dc34-469112fa469b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: LOADING DATASETS\n",
            "================================================================================\n",
            "\n",
            "Loading mentalhealth.csv...\n",
            "  Loaded 97 FAQ entries\n",
            "\n",
            "Loading train.csv...\n",
            "  Loaded 3512 dialogue entries\n",
            "\n",
            "Merging datasets...\n",
            "  Initial merged rows: 3609\n",
            "\n",
            "================================================================================\n",
            "STEP 2: DEDUPLICATION\n",
            "================================================================================\n",
            "\n",
            "Removed 1043 duplicates\n",
            "Rows after dedup: 2566\n",
            "\n",
            "================================================================================\n",
            "STEP 3: CLEANING & FILTERING\n",
            "================================================================================\n",
            "CLEANING DATASET\n",
            "Initial rows: 2566\n",
            "  â†’ Converting to string and normalizing...\n",
            "  â†’ Filtering by length...\n",
            "\n",
            "CLEANING COMPLETE!\n",
            "Final rows: 1522\n",
            "Removed: 1044 (40.7%)\n",
            "Retained: 59.3% of original\n",
            "Time: 0.27s\n",
            "\n",
            "Saved cleaned English dataset to: output/merged_en.csv\n",
            "  Final row count: 1522\n",
            "\n",
            "Sample of cleaned data:\n",
            "                                              prompt  \\\n",
            "0        What does it mean to have a mental illness?   \n",
            "1                    Who does mental illness affect?   \n",
            "2  What are some of the warning signs of mental i...   \n",
            "\n",
            "                                            response  \n",
            "0  Mental illnesses are health conditions that di...  \n",
            "1  Mental illness does can affect anyone, regardl...  \n",
            "2  Symptoms of mental health disorders vary depen...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Function"
      ],
      "metadata": {
        "id": "j_SIQ6efvR91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_translate_one(text, target_lang):\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            return DeepGoogle(source='auto', target=target_lang).translate(text)\n",
        "        except Exception as e:\n",
        "            wait = 2 ** attempt\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                print(f\"Translation error: {e}. Retrying in {wait}s...\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                print(f\"Translation failed after {MAX_RETRIES} attempts. Returning original.\")\n",
        "                return text\n",
        "\n",
        "def translate_batch(texts, target_lang):\n",
        "    out = []\n",
        "    for i in tqdm(range(len(texts)), desc=f\"Translating to {target_lang}\"):\n",
        "        t = texts[i]\n",
        "        out.append(deep_translate_one(t, target_lang))\n",
        "\n",
        "        # Rate limiting\n",
        "        if i % 5 == 0:\n",
        "            time.sleep(SLEEP_BETWEEN_TRANSLATIONS)\n",
        "        if i % 50 == 0:\n",
        "            time.sleep(1)\n",
        "\n",
        "    return out\n",
        "\n",
        "def batch_translate_df(df, target_langs):\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "\n",
        "    for lang in target_langs:\n",
        "        print(f\"\\nðŸŒ Translating to {lang.upper()}...\")\n",
        "\n",
        "        # Get all prompts and responses\n",
        "        prompts = df['prompt'].tolist()\n",
        "        responses = df['response'].tolist()\n",
        "\n",
        "        # Translate\n",
        "        translated_prompts = translate_batch(prompts, lang)\n",
        "        translated_responses = translate_batch(responses, lang)\n",
        "\n",
        "        # Create new rows with translations\n",
        "        for i in range(n):\n",
        "            base = df.iloc[i]\n",
        "            rows.append({\n",
        "                'id': base['id'],\n",
        "                'source': base['source'],\n",
        "                'type': base['type'],\n",
        "                'prompt': translated_prompts[i],\n",
        "                'response': translated_responses[i],\n",
        "                'lang': lang,\n",
        "                'orig_id': base['orig_id'],\n",
        "                'fp': stable_fingerprint(translated_prompts[i] + translated_responses[i])\n",
        "            })\n",
        "\n",
        "        # Brief rest between languages\n",
        "        time.sleep(2)\n",
        "\n",
        "    # Also include original English rows\n",
        "    en_rows = df.to_dict(orient='records')\n",
        "\n",
        "    # Combine\n",
        "    all_rows = en_rows + rows\n",
        "    return pd.DataFrame(all_rows)\n",
        "\n",
        "print(\"Translation functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHkOEfmVvOl6",
        "outputId": "760c55db-d9ae-4d3d-919a-69c638c6ab24"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate Dataset"
      ],
      "metadata": {
        "id": "M-2WMoENv6Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"--\" * 80)\n",
        "print(\"TRANSLATION\")\n",
        "print(\"--\" * 80)\n",
        "\n",
        "print(f\"\\nTranslating dataset into: {TARGET_LANGS}\")\n",
        "print(f\"This will take approximately {len(merged) * len(TARGET_LANGS) * 0.3 / 60:.1f} minutes\")\n",
        "print(f\"(Translating {len(merged)} entries Ã— {len(TARGET_LANGS)} languages)\")\n",
        "\n",
        "multilang = batch_translate_df(merged, TARGET_LANGS)\n",
        "\n",
        "print(f\"\\nTranslation complete!\")\n",
        "print(f\"Total rows (all languages): {len(multilang)}\")\n",
        "print(f\"English: {len(multilang[multilang['lang'] == 'en'])}\")\n",
        "for lang in TARGET_LANGS:\n",
        "    print(f\"{lang.upper()}: {len(multilang[multilang['lang'] == lang])}\")\n",
        "\n",
        "# Save multilingual CSV\n",
        "multilang.to_csv(MERGED_MULTILANG_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"\\nSaved multilingual dataset to: {MERGED_MULTILANG_PATH}\")\n",
        "\n",
        "# SAVE JSONL FORMAT (for fine-tuning)\n",
        "print(\"\\n\" + \"--\" * 80)\n",
        "print(\"SAVE JSONL FORMAT\")\n",
        "print(\"--\" * 80)\n",
        "\n",
        "with open(MERGED_JSONL_PATH, 'w', encoding='utf-8') as fh:\n",
        "    for _, r in multilang.iterrows():\n",
        "        obj = {\n",
        "            'id': r['id'],\n",
        "            'lang': r['lang'],\n",
        "            'prompt': r['prompt'],\n",
        "            'response': r['response']\n",
        "        }\n",
        "        fh.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n",
        "\n",
        "print(f\"\\nSaved JSONL to: {MERGED_JSONL_PATH}\")\n",
        "\n",
        "# Display sample from each language\n",
        "print(\"\\nSample from each language:\")\n",
        "for lang in ['en'] + TARGET_LANGS:\n",
        "    filtered = multilang[multilang['lang'] == lang]\n",
        "    if len(filtered) == 0:\n",
        "        print(f\"\\n[{lang.upper()}]\\n  No samples found.\")\n",
        "        continue\n",
        "    sample = filtered.iloc[0]\n",
        "    print(f\"\\n[{lang.upper()}]\")\n",
        "    print(f\"  Prompt:  {sample['prompt'][:80]}...\")\n",
        "    print(f\"  Response: {sample['response'][:80]}...\")\n",
        "\n",
        "print(\"\\n\" + \"--\" * 80)\n",
        "print(\"ALL PROCESSING COMPLETE!\")\n",
        "print(\"--\" * 80)\n",
        "print(f\"\\nOutput files created in '{OUT_DIR}':\")\n",
        "print(f\"  1. {MERGED_EN_PATH.name} - English only\")\n",
        "print(f\"  2. {MERGED_MULTILANG_PATH.name} - All languages (CSV)\")\n",
        "print(f\"  3. {MERGED_JSONL_PATH.name} - All languages (JSONL)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jPQdITWv0VU",
        "outputId": "4aa8a4a9-fa10-4057-b350-b644ca653026"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "TRANSLATION\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Translating dataset into: ['hi', 'te']\n",
            "This will take approximately 15.2 minutes\n",
            "(Translating 1522 entries Ã— 2 languages)\n",
            "\n",
            "ðŸŒ Translating to HI...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating to hi: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1522/1522 [14:57<00:00,  1.70it/s]\n",
            "Translating to hi: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1522/1522 [28:32<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŒ Translating to TE...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating to te: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1522/1522 [08:27<00:00,  3.00it/s]\n",
            "Translating to te: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1522/1522 [14:06<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translation complete!\n",
            "Total rows (all languages): 4566\n",
            "English: 1522\n",
            "HI: 1522\n",
            "TE: 1522\n",
            "\n",
            "Saved multilingual dataset to: output/merged_multilang.csv\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SAVE JSONL FORMAT\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved JSONL to: output/merged_multilang.jsonl\n",
            "\n",
            "Sample from each language:\n",
            "\n",
            "[EN]\n",
            "  Prompt:  What does it mean to have a mental illness?...\n",
            "  Response: Mental illnesses are health conditions that disrupt a person's thoughts, emotion...\n",
            "\n",
            "[HI]\n",
            "  Prompt:  à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤¬à¥€à¤®à¤¾à¤°à¥€ à¤¹à¥‹à¤¨à¥‡ à¤•à¤¾ à¤•à¥à¤¯à¤¾ à¤®à¤¤à¤²à¤¬ à¤¹à¥ˆ?...\n",
            "  Response: à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤¬à¥€à¤®à¤¾à¤°à¤¿à¤¯à¤¾à¤ à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¥à¤¥à¤¿à¤¤à¤¿à¤¯à¤¾à¤ à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤•à¤¿à¤¸à¥€ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿ à¤•à¥‡ à¤µà¤¿à¤šà¤¾à¤°à¥‹à¤‚, à¤­à¤¾à¤µà¤¨à¤¾à¤“à¤‚, à¤°à¤¿...\n",
            "\n",
            "[TE]\n",
            "  Prompt:  à°®à°¾à°¨à°¸à°¿à°• à°…à°¨à°¾à°°à±‹à°—à±à°¯à°‚ à°…à°‚à°Ÿà±‡ à°à°®à°¿à°Ÿà°¿?...\n",
            "  Response: à°®à°¾à°¨à°¸à°¿à°• à°…à°¨à°¾à°°à±‹à°—à±à°¯à°¾à°²à± à°’à°• à°µà±à°¯à°•à±à°¤à°¿ à°¯à±Šà°•à±à°• à°†à°²à±‹à°šà°¨à°²à±, à°­à°¾à°µà±‹à°¦à±à°µà±‡à°—à°¾à°²à±, à°¸à°‚à°¬à°‚à°§à°¾à°²à± à°®à°°à°¿à°¯à± à°°à±‹à°œà±à°µà°¾...\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ALL PROCESSING COMPLETE!\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Output files created in 'output':\n",
            "  1. merged_en.csv - English only\n",
            "  2. merged_multilang.csv - All languages (CSV)\n",
            "  3. merged_multilang.jsonl - All languages (JSONL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "1KkpA2DrxGaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\" * 80)\n",
        "print(\"DATA STATISTICS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\nOVERALL STATISTICS:\")\n",
        "print(f\"Total rows: {len(multilang)}\")\n",
        "print(f\"Unique prompts: {multilang['prompt'].nunique()}\")\n",
        "print(f\"Unique responses: {multilang['response'].nunique()}\")\n",
        "\n",
        "# Language breakdown\n",
        "print(f\"\\nLANGUAGE BREAKDOWN:\")\n",
        "lang_counts = multilang['lang'].value_counts()\n",
        "for lang, count in lang_counts.items():\n",
        "    pct = (count / len(multilang)) * 100\n",
        "    print(f\"{lang.upper()}: {count:,} rows ({pct:.1f}%)\")\n",
        "\n",
        "# Type breakdown (FAQ vs Dialogue)\n",
        "print(f\"\\nTYPE BREAKDOWN:\")\n",
        "type_counts = multilang['type'].value_counts()\n",
        "for typ, count in type_counts.items():\n",
        "    pct = (count / len(multilang)) * 100\n",
        "    print(f\"{typ.upper()}: {count:,} rows ({pct:.1f}%)\")\n",
        "\n",
        "# Length statistics (English only)\n",
        "en_data = multilang[multilang['lang'] == 'en']\n",
        "prompt_lens = en_data['prompt'].str.len()\n",
        "response_lens = en_data['response'].str.len()\n",
        "\n",
        "print(f\"\\nLENGTH STATISTICS (English):\")\n",
        "print(f\"Prompt length:\")\n",
        "print(f\"Mean: {prompt_lens.mean():.1f} chars\")\n",
        "print(f\"Median: {prompt_lens.median():.1f} chars\")\n",
        "print(f\"Min: {prompt_lens.min()} chars\")\n",
        "print(f\"Max: {prompt_lens.max()} chars\")\n",
        "\n",
        "print(f\"Response length:\")\n",
        "print(f\"Mean: {response_lens.mean():.1f} chars\")\n",
        "print(f\"Median: {response_lens.median():.1f} chars\")\n",
        "print(f\"Min: {response_lens.min()} chars\")\n",
        "print(f\"Max: {response_lens.max()} chars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKEVY7Q7xCqY",
        "outputId": "aa25edb0-aae2-464c-87c4-53fbdeda0001"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "DATA STATISTICS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "OVERALL STATISTICS:\n",
            "Total rows: 4566\n",
            "Unique prompts: 2053\n",
            "Unique responses: 3918\n",
            "\n",
            "LANGUAGE BREAKDOWN:\n",
            "EN: 1,522 rows (33.3%)\n",
            "HI: 1,522 rows (33.3%)\n",
            "TE: 1,522 rows (33.3%)\n",
            "\n",
            "TYPE BREAKDOWN:\n",
            "DIALOGUE: 4,431 rows (97.0%)\n",
            "FAQ: 135 rows (3.0%)\n",
            "\n",
            "LENGTH STATISTICS (English):\n",
            "Prompt length:\n",
            "Mean: 250.0 chars\n",
            "Median: 221.0 chars\n",
            "Min: 12 chars\n",
            "Max: 992 chars\n",
            "Response length:\n",
            "Mean: 585.2 chars\n",
            "Median: 587.0 chars\n",
            "Min: 10 chars\n",
            "Max: 999 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Train, val and test Splits"
      ],
      "metadata": {
        "id": "WPIgUO-XydkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"CREATING TRAIN/VAL/TEST SPLITS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def create_splits(df, lang='en', val_size=0.15, test_size=0.10):\n",
        "    # Filter by language\n",
        "    df_lang = df[df['lang'] == lang].copy()\n",
        "\n",
        "    # First split: train+val vs test\n",
        "    train_val, test = train_test_split(\n",
        "        df_lang,\n",
        "        test_size=test_size,\n",
        "        stratify=df_lang['type'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Second split: train vs val\n",
        "    adjusted_val_size = val_size / (1 - test_size)\n",
        "    train, val = train_test_split(\n",
        "        train_val,\n",
        "        test_size=adjusted_val_size,\n",
        "        stratify=train_val['type'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    return train, val, test\n",
        "\n",
        "# Create splits for each language\n",
        "for lang in ['en'] + TARGET_LANGS:\n",
        "    print(f\"\\nCreating splits for {lang.upper()}...\")\n",
        "\n",
        "    train, val, test = create_splits(multilang, lang=lang)\n",
        "\n",
        "    # Save splits\n",
        "    train.to_csv(OUT_DIR / f\"train_{lang}.csv\", index=False)\n",
        "    val.to_csv(OUT_DIR / f\"val_{lang}.csv\", index=False)\n",
        "    test.to_csv(OUT_DIR / f\"test_{lang}.csv\", index=False)\n",
        "\n",
        "    print(f\"Train: {len(train):,} rows ({len(train)/len(multilang[multilang['lang']==lang])*100:.1f}%)\")\n",
        "    print(f\"Val:   {len(val):,} rows ({len(val)/len(multilang[multilang['lang']==lang])*100:.1f}%)\")\n",
        "    print(f\"Test:  {len(test):,} rows ({len(test)/len(multilang[multilang['lang']==lang])*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nAll splits created and saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo6XX8J4yZQp",
        "outputId": "afe784de-0917-4320-890e-f2b23adc0de4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "CREATING TRAIN/VAL/TEST SPLITS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Creating splits for EN...\n",
            "Train: 1,140 rows (74.9%)\n",
            "Val:   229 rows (15.0%)\n",
            "Test:  153 rows (10.1%)\n",
            "\n",
            "Creating splits for HI...\n",
            "Train: 1,140 rows (74.9%)\n",
            "Val:   229 rows (15.0%)\n",
            "Test:  153 rows (10.1%)\n",
            "\n",
            "Creating splits for TE...\n",
            "Train: 1,140 rows (74.9%)\n",
            "Val:   229 rows (15.0%)\n",
            "Test:  153 rows (10.1%)\n",
            "\n",
            "All splits created and saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing for Model Training"
      ],
      "metadata": {
        "id": "w-ZdmhEJzCSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"--\" * 80)\n",
        "print(\"PREPARING DATA FOR MODEL TRAINING\")\n",
        "print(\"--\" * 80)\n",
        "\n",
        "# Load English training data\n",
        "train_df = pd.read_csv(OUT_DIR / \"train_en.csv\")\n",
        "val_df = pd.read_csv(OUT_DIR / \"val_en.csv\")\n",
        "\n",
        "# Extract inputs and targets\n",
        "train_inputs = train_df[\"prompt\"].tolist()\n",
        "train_targets = train_df[\"response\"].tolist()\n",
        "val_inputs = val_df[\"prompt\"].tolist()\n",
        "val_targets = val_df[\"response\"].tolist()\n",
        "\n",
        "print(f\"\\nTraining data prepared:\")\n",
        "print(f\"Training samples: {len(train_inputs):,}\")\n",
        "print(f\"Validation samples: {len(val_inputs):,}\")\n",
        "\n",
        "# Sample data for verification\n",
        "print(f\"\\nSample training examples:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Input:  {train_inputs[i][:100]}...\")\n",
        "    print(f\"Target: {train_targets[i][:100]}...\")\n",
        "\n",
        "print(\"\\n\" + \"**\" * 80)\n",
        "print(\"DATA PREPARATION COMPLETE\")\n",
        "print(\"**\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpJFls1py_9v",
        "outputId": "8a64df3d-ec9c-4001-bfb2-8e043dc9b899"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PREPARING DATA FOR MODEL TRAINING\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training data prepared:\n",
            "Training samples: 1,140\n",
            "Validation samples: 229\n",
            "\n",
            "Sample training examples:\n",
            "\n",
            "Example 1:\n",
            "Input:  Is this something I should be worried about? Should I do something about it?...\n",
            "Target: The answer depends on how the other areas of your son's life are doing.Is he happy or does he seem h...\n",
            "\n",
            "Example 2:\n",
            "Input:  I have an eating disorder of binging. I've had gastric sleeve surgery. I need help with issues of ab...\n",
            "Target: It can be really frustrating to feel like your counselor is not providing you with the help you need...\n",
            "\n",
            "Example 3:\n",
            "Input:  There's this boy who asked me out awhile ago. I said no because my friends and family would think we...\n",
            "Target: Since you've decided to be with the boy who already showed you his interest, then why not simply tel...\n",
            "\n",
            "****************************************************************************************************************************************************************\n",
            "DATA PREPARATION COMPLETE\n",
            "****************************************************************************************************************************************************************\n"
          ]
        }
      ]
    }
  ]
}